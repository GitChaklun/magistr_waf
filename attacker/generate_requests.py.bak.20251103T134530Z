#!/usr/bin/env python3
"""
generate_requests.py
Генерує набір запитів (атак) до TARGET використовуючи payloads з attacker/payloads.py
Параметри:
  --target  (required) base URL, наприклад http://waf:80
  --out     (required) output csv path, наприклад /opt/results/results.csv
  --workers (optional) number of threads (default 8)
  --repeat  (optional) repeat each payload N times (default 1)
Приклад:
  python generate_requests.py --target http://waf:80 --out /opt/results/results.csv --workers 32 --repeat 50
"""
import argparse, time, csv, uuid, sys, random, hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, quote_plus
import requests

# import user payloads dict
try:
    from attacker.payloads import PAYLOADS
except Exception:
    # fallback: minimal payloads if attacker/payloads.py missing
    PAYLOADS = {
        "sqli": ["1' OR '1'='1", "1 OR 1=1"],
        "xss": ["<script>alert(1)</script>", "<img src=x onerror=alert(1)>"],
        "idor": ["/rest/user/1", "/rest/user/2"]
    }

# Small pool of user agents and header variability
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Mozilla/5.0 (X11; Linux x86_64)",
    "curl/7.68.0",
    "python-requests/2.28.1",
    "sqlmap/1.6.11",
    "Wget/1.21.1"
]

def rand_ip():
    return "{}.{}.{}.{}".format(random.randint(1,254), random.randint(0,255), random.randint(0,255), random.randint(1,254))

def maybe_encode(p):
    """Random small transformations to increase diversity"""
    r = random.random()
    if r < 0.12:
        return quote_plus(p)                # URL-encode
    if r < 0.20:
        return p.replace(" ", "%20")        # space->%20
    if r < 0.26:
        return p + "%00"                    # add null terminator
    if r < 0.32:
        return p + ("X" * random.randint(1,80))  # padding
    return p

def build_headers():
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "X-Forwarded-For": rand_ip(),
        "Accept": "*/*"
    }

def build_request_from_payload(target_base, attack_type, payload):
    """
    Decide how to place payload:
    - idor payloads starting with '/' are used as path (joined to base)
    - else: use query parameter (q or id) depending on attack_type
    """
    if attack_type == "idor":
        # payload likely path; if not, put into query
        if payload.startswith("/"):
            url = urljoin(target_base + ("" if target_base.endswith("/") else "/"), payload.lstrip("/"))
            method = "GET"
            body = None
        else:
            url = urljoin(target_base + "/", "")
            url = url + "?id=" + quote_plus(payload)
            method = "GET"
            body = None
    elif attack_type == "sqli":
        # choose id or q param randomly; prefer id for SQLi attempts
        if random.random() < 0.6:
            url = target_base + "/?id=" + quote_plus(payload)
        else:
            url = target_base + "/?q=" + quote_plus(payload)
        method = "GET"
        body = None
    elif attack_type == "xss":
        # place in q param or path
        if random.random() < 0.7:
            url = target_base + "/?q=" + quote_plus(payload)
        else:
            url = target_base + "/?name=" + quote_plus(payload)
        method = "GET"
        body = None
    else:
        # fallback
        url = target_base + "/?q=" + quote_plus(payload)
        method = "GET"
        body = None

    return method, url, body

def worker(task):
    """
    task: dict with keys: run_id, attack_id, attack_type, payload, target
    returns a dict with result fields
    """
    run_id = task["run_id"]
    attack_id = task["attack_id"]
    attack_type = task["attack_type"]
    raw_payload = task["payload"]
    target = task["target"]

    payload = maybe_encode(raw_payload)
    headers = build_headers()
    method, url, body = build_request_from_payload(target, attack_type, payload)

    # perform request and measure latency
    start = time.perf_counter()
    try:
        resp = requests.request(method, url, headers=headers, timeout=10, data=body)
        status = resp.status_code
        resp_text = resp.text or ""
        response_len = len(resp_text)
        # compute simple body hash
        body_hash = hashlib.sha1(resp_text.encode('utf-8', errors='ignore')).hexdigest()
        notes = ""
    except requests.exceptions.RequestException as e:
        status = 0
        response_len = 0
        body_hash = ""
        notes = str(e)
    end = time.perf_counter()
    latency_ms = round((end - start) * 1000, 3)

    # blocked detection heuristic: if status in 403/406/503 or body contains certain strings
    blocked = 1 if status in (403, 406, 503, 501) else 0
    # additional heuristics: WAF might return "Access Denied" etc
    try:
        if resp is not None and isinstance(resp, requests.Response):
            lb = (resp.text or "").lower()
            if "access denied" in lb or "blocked" in lb or "forbidden" in lb:
                blocked = 1
    except Exception:
        pass

    return {
        "run_id": run_id,
        "attack_id": attack_id,
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S%z"),
        "attack_type": attack_type,
        "url": url,
        "payload": raw_payload,
        "method": method,
        "headers": str(headers),
        "status_code": status,
        "blocked": blocked,
        "latency_ms": latency_ms,
        "response_len": response_len,
        "body_hash": body_hash,
        "notes": notes
    }

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--target", required=True, help="Base URL, e.g. http://waf:80 or http://localhost:8082")
    parser.add_argument("--out", required=True, help="Output CSV full path, e.g. /opt/results/results.csv")
    parser.add_argument("--workers", type=int, default=8)
    parser.add_argument("--repeat", type=int, default=1, help="Repeat each payload N times")
    args = parser.parse_args()

    TARGET = args.target.rstrip("/")
    OUT = args.out
    WORKERS = max(1, args.workers)
    REPEAT = max(1, args.repeat)

    # Build flat list of tasks (attack_type, payload) from PAYLOADS dict
    tasks = []
    run_id = str(uuid.uuid4())[:8]
    attack_counter = 0

    # For balanced coverage: for each attack type, for each payload in its pool, append REPEAT tasks
    for attack_type, pool in PAYLOADS.items():
        for p in pool:
            for r in range(REPEAT):
                attack_counter += 1
                tasks.append({
                    "run_id": run_id,
                    "attack_id": attack_counter,
                    "attack_type": attack_type,
                    "payload": p,
                    "target": TARGET
                })

    random.shuffle(tasks)  # randomize execution order

    # prepare CSV
    fieldnames = ["run_id","attack_id","timestamp","attack_type","url","payload","method","headers","status_code","blocked","latency_ms","response_len","body_hash","notes"]
    # ensure directory exists for OUT
    import os
    os.makedirs(os.path.dirname(OUT), exist_ok=True)

    print(f"[generate_requests] Target={TARGET} tasks={len(tasks)} workers={WORKERS}")

    # execute in ThreadPoolExecutor
    results = []
    with ThreadPoolExecutor(max_workers=WORKERS) as ex, open(OUT, "w", newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        futures = {ex.submit(worker, t): t for t in tasks}
        completed = 0
        start_all = time.time()
        for fut in as_completed(futures):
            res = fut.result()
            writer.writerow(res)
            completed += 1
            if completed % 50 == 0 or completed == len(tasks):
                elapsed = time.time() - start_all
                print(f"[generate_requests] completed {completed}/{len(tasks)} elapsed={elapsed:.1f}s")
        csvfile.flush()

    print(f"[generate_requests] Finished. Wrote {OUT} ({len(tasks)} rows)")

if __name__ == "__main__":
    main()

